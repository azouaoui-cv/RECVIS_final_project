# Title of papers

> [paper](https://arxiv.org/pdf/1707.09472.pdf) 

> [code](https://github.com/jpeyre/unrel)

## TLDR

This paper introduces a novel approach for modeling visual relations between pairs of objects. Here a relation is a triplet of the form (*subject, predicate, object*) for instance: *car under elephant*.

## Aim

> objectives, contributions


* This is a need for a method that first detect individual entities in an image and then model their relations, otherwise the large number of relations grows combinatorially. A method that can share knowledge among similar relations is therefore required.
* Weakly-learning setup is useful when only image-level annotations for object relations are available.

1. Design strong yet flexible visual features that encode the apperance and spatial configuration for pairs of objects.
2. Propose a weakly-supervised discriminative clustering model to learn relations from image-level labels only.
3. Introduce a new dataset of unusual relations with an exhaustive annotation, the enables accurate evaluation of visual relation retrieval.

## Methods

> whats new in their approaches

* Visual representation of relations. Given the triplet $$t = (s, r, o)$$, we have the object bounding boxes ($$o_s = [x_s, y_s, w_s, h_s], o_o = [x_o, y_o, w_o, h_o]$$) and the individual appearance of each object. The spatial configuration is encoded with a 6-dimensional vector: $$r(o_s, o_o) = [r_1, r_2, r_3, r_4, r_5, r_6]$$ (see eq. 1 for details) where $$r_1$$ and $$r_2$$ represent the renormalized translation between the two boxes, $$r_3$$ is the ratio of box sizes, $$r_4$$ is the overlap between boxes, and $$r_5, r_6$$ encode the aspect ratio of each box respectively.

* This feature vector is discretized into $$k$$ bins where it is assumed that the spatial configurations are generated by a mixture of $$k$$ Gaussians that are fitted on the training data.

* Appearance features are given by the fc7 output of a Fast-RCNN trained to detect individual objects (with VGG-16 pre-trained on ImageNet), then PCA to reduce from 4096 to 300.

* The visual feature is a concatenation of the spatial configuration and the appearance of objects. In the experiments it has 1000 dimensions.


## Technical details

> mathematical notations, proofs

* A triplet score can be computed at test time for a pair of boxes relative to a triplet t as the combination of the score returned by the classifier $$w_r$$ for predicate $$r$$ for the visual representation of the pair of boxes and the object class likelihoods returned by the object detector. A language model score can be added.

* Franke-Wolfe algorithm variant is used to optimize the cost on pairs of objects in the training set.

> setups

* Fully-supervised setup: each relation is associated with a pair of object boxes in the image, ridge regression is used to train a multi-way relation classifier to assign a relation to a given visual feature.

* Weakly-supervised setup: weakly-supervised discriminative clustering object where latent variables are introduced to model which pairs of objects participate in the relation. A classifier is trained for each predicate $$r$$ and latent variables are constrained based on the weak annotations. Relation classifiers are shared among object categories. Hence they are able to generalize to unseen triplets.


> parameterizing

* Weakly-supervised setup requires $$Z$$ to be a latent assignment matrix and it has to be learnt together with the classifiers. Valid latent assignment matrices must satisfy the multiclass constraint $$Z 1_R = 1_N$$ which assigns a pair of objects to one and only one predicate $$r$$.


## Experiments - results

> comparable experiments on typical dataset/environments...

1. Visual Relationship Detection dataset
  * 4000/1000 train/test
  * 10% of the test triplets are not seen at training (allow evaluation of zero-shot learning)
  * 30k annotated triplets ~ 7.5 relations per image
  * 100 object / 70 predicates
  * 6672 different annotation triplets
  * Evaluation setup:
    * ``recall@x``: proportion of ground truth pairs among the x top scored candidate pairs in each image.
    * Predicate detection: candidate pair of boxes are ground truth boxes (i.e. predicate classifier quality).
    * Check whether the candidate boxes intersect the ground truth boxes:
      * relationship detection: both subject and object boxes match
      * phrase detection: the union of them match.
    * Location criteria: IoU = 0.5
    * spatial representation [S]
    * apperance representation [A]
    * spatial + appearance [S+A]
    * Combine language model from prior work 31.

  * Weak supervision: the good results compared to previous work can be explained by the strength of the visual features that generalized well to unseen triplets.
  * Noisy full-supervised setup: surprisingly high (dataset bias where many images have only two prominent objects involved in a specific relation (more than half of the triplets).


2. UnRel Dataset (unusual relations)
  * Web images from unusual language triplet queries (e.g. *dog ride bike*)
  * Unusual triplets are rare -> generalization matters.
  * 1000 images / 76 triplet queries
  * Weakly-supervised method outperforms the strong baselines that are fully supervised.

> breakthroughs

1. The results obtained using only spatial features suggests that the spatial clusters group pairs of objects in similar relations.

2. Generalization to unseen triplets is good

3. Training from weak-imag elevel is possible and results only in a small loss of accuracy compared to training from fully supervised data.

## My thoughts and takeaways

> pros

> cons

> related stuff

* How to disambiguate situations such as *car under elephant* versus *elephant on top of car*?


## Metadata

> easily grep-able data: tags, reading depth

> bibtex: author list, title, published year, journal, URL, etc
```
@InProceedings{Peyre17,
    author      = "Peyre, Julia and Laptev, Ivan and Schmid, Cordelia and Sivic, Josef",
    title       = "Weakly-supervised learning of visual relations",
    booktitle   = "ICCV",
    year        = "2017"
}
```
